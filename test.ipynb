{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32c8f18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from huggingface_hub import InferenceClient\n",
    "# from datetime import datetime\n",
    "\n",
    "# client = InferenceClient(\n",
    "#     api_key=os.environ[\"HF_TOKEN\"],\n",
    "# )\n",
    "\n",
    "# # System prompt for pseudo code generation\n",
    "# SYSTEM_PROMPT = \"\"\"You are an expert pseudo code generator. Your task is to:\n",
    "# 1. Analyze control narrative documents\n",
    "# 2. Extract key processes and workflows\n",
    "# 3. Convert them into clear, well-structured pseudo code\n",
    "# 4. Use proper indentation and logical flow\n",
    "# 5. No actual code syntax - only pseudo code notation\n",
    "# 6. No explanations or additional text - only the pseudo code\n",
    "# 7. And no extras, just pseudo code based on the given information.\"\"\"\n",
    "\n",
    "# user_input = \"Write pseudo code based on this control narrative document:\"\n",
    "\n",
    "# # Convert bytes to string\n",
    "# full_text_str = \"\\n\".join([text.decode(\"utf-8\") if isinstance(text, bytes) else text for text in full_text])\n",
    "\n",
    "# # print(\"\\n\" + \"=\"*80)\n",
    "# # print(\"PSEUDO CODE GENERATION - HuggingFace API\")\n",
    "# # print(\"=\"*80)\n",
    "# # print(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "# # print(f\"Model: openai/gpt-oss-20b\")\n",
    "# # print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# completion = client.chat.completions.create(\n",
    "#     model=\"openai/gpt-oss-20b\",\n",
    "#     messages=[\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": SYSTEM_PROMPT + \"\\n\\n\" + user_input + \"\\n\\n\" + full_text_str\n",
    "#         }\n",
    "#     ],\n",
    "#     max_tokens=2048,\n",
    "#     temperature=0.7,\n",
    "# )\n",
    "\n",
    "# response_text = completion.choices[0].message.content\n",
    "\n",
    "# print(response_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b1eb0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "\n",
    "doc = pymupdf.open(\"document.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c5826a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_blocks = []\n",
    "image_blocks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cd00966",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_content(pdf_path):\n",
    "    doc = pymupdf.open(pdf_path)\n",
    "\n",
    "    text_collection = []\n",
    "    image_collection = []\n",
    "\n",
    "    for page_num, page in enumerate(doc):\n",
    "        page_dict = page.get_text(\"dict\")\n",
    "\n",
    "        for block_id, block in enumerate(page_dict[\"blocks\"]):\n",
    "            if block[\"type\"] == 0:\n",
    "                text = \" \".join(\n",
    "                    span[\"text\"]\n",
    "                    for line in block[\"lines\"]\n",
    "                    for span in line[\"spans\"]\n",
    "                ).strip()\n",
    "\n",
    "                if text:\n",
    "                    text_collection.append({\n",
    "                        \"id\": f\"text_p{page_num}_b{block_id}\",\n",
    "                        \"content\": text,\n",
    "                        \"metadata\": {\n",
    "                            \"page\": page_num,\n",
    "                            \"bbox\": block[\"bbox\"],\n",
    "                            \"modality\": \"text\"\n",
    "                        }\n",
    "                    })\n",
    "        \n",
    "        for img_id, img in enumerate(page.get_images(full=True)):\n",
    "            xref = img[0]\n",
    "            base = doc.extract_image(xref)\n",
    "\n",
    "            image_collection.append({\n",
    "                \"id\": f\"image_p{page_num}_{img_id}\",\n",
    "                \"image_bytes\": base[\"image\"],\n",
    "                \"format\": base[\"ext\"],\n",
    "                \"metadata\": {\n",
    "                    \"page\": page_num,\n",
    "                    \"xref\": xref,\n",
    "                    \"modality\": \"image\"\n",
    "                }\n",
    "            })\n",
    "    return text_collection, image_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77565908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "845 text chunks\n",
      "46 images\n"
     ]
    }
   ],
   "source": [
    "texts, images = extract_pdf_content(\"document.pdf\")\n",
    "\n",
    "print(len(texts), \"text chunks\")\n",
    "print(len(images), \"images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cef19ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.optim as optim\n",
      "\n",
      "class SimpleNN(nn.Module):\n",
      "    def __init__(self, input_size, hidden_size, output_size):\n",
      "        super(SimpleNN, self).__init__()\n",
      "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
      "        self.relu = nn.ReLU()\n",
      "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.fc1(x)\n",
      "        x = self.relu(x)\n",
      "        x = self.fc2(x)\n",
      "        return x\n",
      "\n",
      "# Example usage\n",
      "input_size = 10\n",
      "hidden_size = 5\n",
      "output_size = 1\n",
      "model = SimpleNN(input_size, hidden_size, output_size)\n",
      "criterion = nn.MSELoss()\n",
      "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
      "\n",
      "# Training loop (example)\n",
      "for epoch in range(100):\n",
      "    # Generate random data\n",
      "    inputs = torch.randn(32, input_size)\n",
      "    targets = torch.randn(32, output_size)\n",
      "\n",
      "    # Forward pass\n",
      "    outputs = model(inputs)\n",
      "    loss = criterion(outputs, targets)\n",
      "\n",
      "    # Backward pass and optimize\n",
      "    optimizer.zero_grad()\n",
      "    loss.backward()\n",
      "    optimizer.step()\n",
      "\n",
      "    if (epoch+1) % 10 == 0:\n",
      "        print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "llm = ChatMistralAI(\n",
    "    model=\"devstral-2512\",\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "response = llm.invoke(\"You are just coding bot, so dont give any other content just asked code. write code for simple neural network in pytorch\")\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8ffb602",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-26 23:00:47.807208: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-12-26 23:00:47.807497: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-26 23:00:47.851041: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-26 23:00:49.487321: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-26 23:00:49.488530: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    }
   ],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "164162fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"document.pdf\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c4904e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200\n",
    ")\n",
    "chunks = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ee97de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33367/661755571.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(chunks, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e00e396",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatMistralAI(model=\"devstral-2512\", temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3df8fefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a STRICT PSEUDOCODE GENERATOR.\n",
    "\n",
    "You MUST FOLLOW THESE RULES EXACTLY:\n",
    "\n",
    "1. OUTPUT ONLY REAL PROCESS / LOGIC / CONTROL STEPS.\n",
    "2. NEVER convert titles, headers, document names, revision info, dates, or metadata\n",
    "   into pseudocode. IGNORE:\n",
    "   - Title page\n",
    "   - Revision history\n",
    "   - Version numbers\n",
    "   - Dates\n",
    "   - Table of contents\n",
    "   - Section headings\n",
    "   - Descriptive paragraphs\n",
    "   - Narratives\n",
    "3. DO NOT OUTPUT STRINGS. DO NOT print document text. DO NOT use quotes.\n",
    "4. Extract ONLY actionable logic:\n",
    "   - controls\n",
    "   - decision conditions\n",
    "   - sequences\n",
    "   - state handling\n",
    "   - alarms\n",
    "   - inputs / outputs\n",
    "   - transitions\n",
    "5. Represent behavior, processes, state transitions and operations ONLY.\n",
    "6. ALL CONTROL KEYWORDS MUST BE CAPITAL:\n",
    "   BEGIN, END, IF, ELSE, ELSE IF, FOR, WHILE, RETURN, FUNCTION\n",
    "7. Use meaningful variable names that match process names in the document.\n",
    "8. Use indented, clean, algorithmic pseudocode.\n",
    "9. No explanation, no text, no narration. Only pseudocode.\n",
    "10. If the document only contains descriptive text, extract logic implied within it.\n",
    "11. If something is not logical or executable, ignore it.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "TASK: {input}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7e46c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "rag_chain = (RunnableParallel( context=retriever, input=RunnablePassthrough() ) | prompt | llm )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d338fed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided document content, here's the extracted pseudocode for the Process Automation System (PAS) control logic:\n",
      "\n",
      "```\n",
      "FUNCTION InitializeChromatographySkid(skidId)\n",
      "    BEGIN\n",
      "        SET skidState TO \"Initializing\"\n",
      "        LOAD recipeParameters FOR skidId\n",
      "        ASSIGN feedSUSV FROM recipeFormula\n",
      "        ASSIGN collectionSUSV FROM recipeFormula\n",
      "        SET skidState TO \"Idle\"\n",
      "    END\n",
      "\n",
      "FUNCTION StartChromatographyPhase(skidId)\n",
      "    BEGIN\n",
      "        IF skidState == \"Idle\" THEN\n",
      "            SET skidState TO \"Startup\"\n",
      "            SEND startCommand TO skidId\n",
      "            MONITOR batchStatus\n",
      "            SET skidState TO \"Feed\"\n",
      "        ELSE\n",
      "            RAISE \"Skid not in idle state\"\n",
      "        END IF\n",
      "    END\n",
      "\n",
      "FUNCTION MonitorBatchStatus(skidId)\n",
      "    BEGIN\n",
      "        WHILE skidState != \"Shutdown\" AND skidState != \"Abort\" DO\n",
      "            IF operatorAcknowledgementReceived THEN\n",
      "                SET skidState TO \"Shutdown\"\n",
      "            ELSE IF upstreamSkidState == \"Idle\" THEN\n",
      "                SET skidState TO \"Shutdown\"\n",
      "            ELSE IF pauseConditionMet THEN\n",
      "                SET skidState TO \"Pause\"\n",
      "            END IF\n",
      "        END WHILE\n",
      "    END\n",
      "\n",
      "FUNCTION HandlePauseState(skidId)\n",
      "    BEGIN\n",
      "        IF skidState == \"Pause\" THEN\n",
      "            WAIT FOR resumeSignal\n",
      "            IF resumeSignalReceived THEN\n",
      "                SET skidState TO \"Feed\"\n",
      "            ELSE IF timeoutOccurred THEN\n",
      "                SET skidState TO \"Abort\"\n",
      "            END IF\n",
      "        END IF\n",
      "    END\n",
      "\n",
      "FUNCTION ShutdownChromatography(skidId)\n",
      "    BEGIN\n",
      "        SET skidState TO \"Shutdown\"\n",
      "        SEND shutdownCommand TO skidId\n",
      "        WAIT FOR drawDownComplete\n",
      "        SET skidState TO \"Idle\"\n",
      "    END\n",
      "\n",
      "FUNCTION AbortChromatography(skidId)\n",
      "    BEGIN\n",
      "        SET skidState TO \"Abort\"\n",
      "        SEND abortCommand TO skidId\n",
      "        SET skidState TO \"Idle\"\n",
      "    END\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke(\"Generate pseudocode based on the document content\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876f2fbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "common_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
